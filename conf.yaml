#  ============== LLM Backend Settings ===================

# Provider of LLM. Choose either "ollama" or "memgpt"
# "ollama" for any OpenAI Compatible backend. "memgpt" requires setup
LLM_PROVIDER: "ollama"


# Ollama & OpenAI Compatible inference backend
ollama:
  # BASE_URL: "http://localhost:11434"
  BASE_URL: "http://localhost:11434/v1"
  LLM_API_KEY: "somethingelse"
  ORGANIZATION_ID: "org_eternity"
  PROJECT_ID: "project_glass"
  ## LLM name
  MODEL: "llama3.1:latest"
  # system prompt is at the very end of this file
  VERBOSE: False



# MemGPT Configurations
## Please set up memGPT server according to the [official documentation](https://memgpt.readme.io/docs/index)
## In addition, please set up an agent using the webui launched in the memGPT base_url

memgpt:
  BASE_URL: "http://localhost:8283"

  # You will find admin server password in memGPT console output. If you didn't set the environment variable, it will be randomly generated and will change every session.
  ADMIN_TOKEN: ""

  # The ID of the agent to send the message to.
  AGENT_ID: ""
  VERBOSE: True



# ============== Live2D front-end Settings ==============

LIVE2D_MODEL: "shizuku-local"

#  ============== Voice Interaction Settings ==============

# === Automatic Speech Recognition ===
VOICE_INPUT_ON: True



# speech to text model options: "Faster-Whisper", "WhisperCPP", "Whisper", "AzureASR", "FunASR", "GroqWhisperASR"
ASR_MODEL: "Faster-Whisper"

# Faster whisper config
Faster-Whisper:
  model_path: "distil-large-v3"
  download_root: "asr/models"
  language: "en"
  device: "cuda" # cpu, cuda, or auto. faster-whisper doesn't support mps

WhisperCPP:
  # all available models are listed on https://abdeladim-s.github.io/pywhispercpp/#pywhispercpp.constants.AVAILABLE_MODELS
  model_name: "small"
  model_dir: "asr/models"
  print_realtime: False
  print_progress: False
  
  language: "en" # en, zh, auto, 

Whisper:
  name: "small"
  download_root: "asr/models"
  device: "cuda"

# ============== Text to Speech ==============
TTS_ON: True
# text to speech model options: "AzureTTS", "pyttsx3TTS", "edgeTTS", "barkTTS", "cosyvoiceTTS", "meloTTS", "piperTTS"
TTS_MODEL: "edgeTTS"

# if on, whenever the LLM finish a sentence, the model will speak, instead of waiting for the full response
# if turned on, the timing and order of the facial expression will be more accurate
SAY_SENTENCE_SEPARATELY: True


#  ============== Translate (to only change the language for TTS) ==============
# Like... you speak and read the subtitles in English, and the TTS speaks Japanese or that kind of things

TRANSLATE_AUDIO: False
TRANSLATE_PROVIDER: "DeepLX"

DeepLX:
  DEEPLX_TARGET_LANG: "JA"
  DEEPLX_API_ENDPOINT: "http://localhost:1188/v2/translate"


#  ============== Other Settings ==============


# Print debug info
VERBOSE: False

# Exit phrase
EXIT_PHRASE: "exit."


# ============== Prompts ==============

# Name of the persona you want to use. 
# All persona files are stored as txt in 'prompts/persona' directory. 
# You can add persona prompt by adding a txt file in the prompts/persona folder and switch to it by enter the file name in here.
# some options: "en_sarcastic_neuro", "zh_翻译腔"
PERSONA_PATH: "en_stella.txt" # or if you rather edit persona prompt below, leave it blank ...
PERSONA_NAME: "Stella"

# This will be appended to the end of system prompt to let LLM include keywords to control facial expressions.
# Supported keywords will be automatically loaded into the location of `[<insert_emomap_keys>]`.
LIVE2D_Expression_Prompt: "live2d_expression_prompt"



